{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir, remove, rename, replace\n",
    "from os.path import isfile, join, isdir\n",
    "import filecmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = ['date', 'id', 'time', 'lat', 'lon', 'h', 'WGS', 'Ml', 'station', 'P/S', 'zeros', 'datetime', 'duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_files(path: str, ending: str) -> [str]:    \n",
    "    '''\n",
    "    Fetches all files in the given path  ending with '.bul'\n",
    "    '''\n",
    "    if path[-1] != '/':\n",
    "        path += '/'\n",
    "    files = []\n",
    "    try:\n",
    "        files = [f for f in listdir(path) if isfile(join(path, f))] \n",
    "    except:\n",
    "        print(\"Trouble opening a directory\")\n",
    "        return files\n",
    "    return [path + f for f in files if f[-len(ending):] == ending]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_files(files: [str]):\n",
    "    ''' Tries to open all the given files and creates pandas dataframes'''\n",
    "    dataframes = []\n",
    "    counter = 0\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_csv(f, delimiter=';', header=None, names=COLUMNS, index_col=False)\n",
    "            dataframes.append(df)\n",
    "            df['file'] = counter\n",
    "            counter += 1\n",
    "        except e:\n",
    "            print('Chyba při čtení souboru', f)\n",
    "            \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unnecessary_columns(df, cols: [str]):\n",
    "    ''' Drops the fiven columns from the dataframe'''\n",
    "    try:\n",
    "        return df.drop(cols, axis=1)\n",
    "    except e:\n",
    "        print('Could not delete some these columns:', cols, 'Returning unchanged dataframe')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_coordinates(df):\n",
    "    ''' Merges 'lat' and 'lon' columns into 'coordinates' column and then drops the original ones '''\n",
    "    df['coordinates'] = df['lat'].str.strip() + \" \" + df['lon'].str.strip()\n",
    "    return df.drop(['lat', 'lon'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Earthquake:\n",
    "    def __init__(self, coordinates, earthquake_id, order_number):\n",
    "        self.__coordinates = coordinates\n",
    "        self.__earthquake_id = earthquake_id\n",
    "        self.__order_number = order_number\n",
    "        self.__records = []\n",
    "        self.__output = pd.DataFrame(columns = ['STA', 'P', 'S', 'S-P'])\n",
    "        self.__buffer = []\n",
    "\n",
    "    \n",
    "    @property \n",
    "    def coordinates(self) -> str:\n",
    "        return self.__coordinates\n",
    "    \n",
    "    \n",
    "    @property \n",
    "    def earthquake_id(self) -> int:\n",
    "        return self.__earthquake_id\n",
    "    \n",
    "    \n",
    "    @property \n",
    "    def order_number(self) -> int:\n",
    "        return self.__order_number\n",
    "    \n",
    "    \n",
    "    def add_record(self, record: list):\n",
    "        ''' Adds a record to the \"self.__records\" list '''\n",
    "        self.__records.append(record.drop(['coordinates']))\n",
    "        \n",
    "    \n",
    "    def convert_to_pandas_df(self):\n",
    "        ''' Converts a normal list to pandas dataframe '''\n",
    "        self.__records = pd.DataFrame(self.__records)\n",
    "    \n",
    "    \n",
    "    def process_records(self):\n",
    "        ''' Processes the records and transforms them into an output-like format list '''\n",
    "        self.__records.datetime = self.__records.datetime - self.__records.datetime.min()\n",
    "        self.__records['zero_time'] = self.__records.datetime.astype(np.int64) // 10**6\n",
    "        self.__records = self.__records.sort_values('zero_time')\n",
    "\n",
    "        for row in self.__records.iterrows():\n",
    "            self.process_line(row[1])\n",
    "            \n",
    "        size = len(self.__buffer)\n",
    "        for i in range(size):\n",
    "            print('S value before a P value at', self.__earthquake_id)\n",
    "            self.process_line(self.__buffer[i])\n",
    "        \n",
    "        \n",
    "    def process_line(self, line: list):\n",
    "        ''' Tranfrorms a single line to an output-like format and saves it to the \"self.__output\" lists'''\n",
    "        if line['P/S'].strip() == 'P':\n",
    "            output_line = {}\n",
    "            output_line['STA'] = line.station\n",
    "            output_line['P'] = line.zero_time\n",
    "            output_line['S'] = 99999\n",
    "            output_line['S-P'] = 99999\n",
    "            self.__output = self.__output.append(output_line, ignore_index = True)\n",
    "            \n",
    "        elif line['P/S'].strip() == 'S':\n",
    "            station = line.station\n",
    "            if len(self.__output[self.__output.STA == station].index.values) == 0:\n",
    "                self.__buffer.append(line)\n",
    "                return\n",
    "            index = self.__output[self.__output.STA == station].index.values[0]\n",
    "            self.__output.loc[index]['S'] = line.zero_time\n",
    "            self.__output.loc[index]['S-P'] = line.zero_time - self.__output.loc[index]['P']\n",
    "            \n",
    "    def create_output_files(self, path):\n",
    "        ''' Creates output files from the 'self.__output' dataframe '''\n",
    "        if path[-1] != '/':\n",
    "            path += '/'\n",
    "        complete_path = path + str(self.earthquake_id) + '_' + str(self.order_number) + '.csv'\n",
    "        print('output to', complete_path)\n",
    "        self.__output.to_csv(complete_path, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_groups(df):\n",
    "    ''' Divides the data into groups based on the 'id' column and then into subgroups based on the 'coordinates' column '''\n",
    "    groups = dict.fromkeys(df.id.unique())\n",
    "#     eq_number = 0\n",
    "    for key in groups.keys():\n",
    "#         eq_number += 1\n",
    "        tmp = df[df.id == key]\n",
    "        subgroups = dict.fromkeys([key for key, _ in tmp.groupby(['coordinates'])])\n",
    "        order_number = 0\n",
    "        for row in tmp.iterrows():\n",
    "            if subgroups[row[1]['coordinates']] == None:\n",
    "                subgroups[row[1]['coordinates']] = Earthquake(row[1]['coordinates'], str(key)[4:], order_number)\n",
    "                order_number += 1\n",
    "            subgroups[row[1]['coordinates']].add_record(row[1])\n",
    "        groups[key] = subgroups\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_data(df):\n",
    "    groups = make_groups(df)\n",
    "    for group in groups.values():\n",
    "        for earthquake in group.values():\n",
    "            earthquake.convert_to_pandas_df()\n",
    "            earthquake.process_records()\n",
    "            earthquake.create_output_files('../output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_files(path):\n",
    "    ''' Compares the dupplicate files, if they are the same then removes one and renames the other '''\n",
    "    if path[-1] != '/':\n",
    "            path += '/'\n",
    "    files = fetch_files(path, '_0.csv')    \n",
    "    for file in files:\n",
    "        if isfile(file[0:-6] + \"_1.csv\"):\n",
    "            print(file)\n",
    "            if filecmp.cmp(file[0:-6] + \"_0.csv\", file[0:-6] + \"_1.csv\"):\n",
    "                print(\"Files are the same => removing \")\n",
    "                remove(file[0:-6] + \"_1.csv\")\n",
    "                if isfile(file[0:-6] + \"_0.csv\"):\n",
    "                    replace(file[0:-6] + \"_0.csv\", file[0:-6] + \".csv\")\n",
    "            else:\n",
    "                print('Files are different => keeping both versions')\n",
    "        else:\n",
    "            if isfile(file):\n",
    "                replace(file, file[0:-6] + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    df = pd.concat(open_files(fetch_files('../data', '.bul')))\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = merge_coordinates(df)\n",
    "    cols = ['h', 'WGS', 'Ml', 'date', 'time', 'zeros', 'duration', 'file']\n",
    "    df = drop_unnecessary_columns(df, cols)\n",
    "    df['station'] = df['station'].str.strip()\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], format=\" %Y-%m-%d %H:%M:%S.%f\")\n",
    "    divide_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output to ../output/4746_0.csv\n",
      "output to ../output/4763_0.csv\n",
      "output to ../output/4799_0.csv\n",
      "output to ../output/4801_0.csv\n",
      "output to ../output/4812_0.csv\n",
      "output to ../output/4812_1.csv\n",
      "output to ../output/4829_0.csv\n",
      "output to ../output/4877_0.csv\n",
      "output to ../output/4902_0.csv\n",
      "output to ../output/4905_0.csv\n",
      "output to ../output/4923_1.csv\n",
      "output to ../output/4923_0.csv\n",
      "output to ../output/5008_1.csv\n",
      "output to ../output/5008_0.csv\n",
      "output to ../output/5015_0.csv\n",
      "output to ../output/5017_0.csv\n",
      "output to ../output/5048_0.csv\n",
      "output to ../output/5048_1.csv\n",
      "output to ../output/5051_0.csv\n",
      "output to ../output/5051_1.csv\n",
      "output to ../output/5052_0.csv\n",
      "output to ../output/5052_1.csv\n",
      "output to ../output/5054_0.csv\n",
      "output to ../output/5055_0.csv\n",
      "output to ../output/5055_1.csv\n",
      "output to ../output/5058_1.csv\n",
      "output to ../output/5058_0.csv\n",
      "S value before a P value at 5060\n",
      "output to ../output/5060_1.csv\n",
      "S value before a P value at 5060\n",
      "output to ../output/5060_0.csv\n",
      "output to ../output/5065_0.csv\n",
      "output to ../output/5065_1.csv\n",
      "output to ../output/5066_1.csv\n",
      "output to ../output/5066_0.csv\n",
      "output to ../output/5069_1.csv\n",
      "output to ../output/5069_0.csv\n",
      "output to ../output/5080_0.csv\n",
      "output to ../output/5080_1.csv\n",
      "output to ../output/5087_1.csv\n",
      "output to ../output/5087_0.csv\n",
      "output to ../output/5122_1.csv\n",
      "output to ../output/5122_0.csv\n",
      "output to ../output/5138_1.csv\n",
      "output to ../output/5138_0.csv\n",
      "output to ../output/5158_1.csv\n",
      "output to ../output/5158_0.csv\n",
      "output to ../output/5207_1.csv\n",
      "output to ../output/5207_0.csv\n",
      "output to ../output/5228_1.csv\n",
      "output to ../output/5228_0.csv\n",
      "output to ../output/5253_0.csv\n",
      "output to ../output/5253_1.csv\n",
      "output to ../output/5262_1.csv\n",
      "output to ../output/5262_0.csv\n",
      "output to ../output/5278_1.csv\n",
      "output to ../output/5278_0.csv\n",
      "output to ../output/5287_1.csv\n",
      "output to ../output/5287_0.csv\n",
      "output to ../output/5372_1.csv\n",
      "output to ../output/5372_0.csv\n",
      "output to ../output/5399_1.csv\n",
      "output to ../output/5399_0.csv\n",
      "output to ../output/5403_1.csv\n",
      "output to ../output/5403_0.csv\n",
      "output to ../output/5406_1.csv\n",
      "output to ../output/5406_0.csv\n",
      "output to ../output/5421_1.csv\n",
      "output to ../output/5421_0.csv\n",
      "output to ../output/5437_1.csv\n",
      "output to ../output/5437_0.csv\n",
      "output to ../output/5540_1.csv\n",
      "output to ../output/5540_0.csv\n",
      "output to ../output/5541_1.csv\n",
      "output to ../output/5541_0.csv\n",
      "output to ../output/5563_1.csv\n",
      "output to ../output/5563_0.csv\n",
      "output to ../output/5564_1.csv\n",
      "output to ../output/5564_0.csv\n",
      "output to ../output/5679_1.csv\n",
      "output to ../output/5679_0.csv\n",
      "output to ../output/5697_1.csv\n",
      "output to ../output/5697_0.csv\n",
      "output to ../output/5745_1.csv\n",
      "output to ../output/5745_0.csv\n",
      "output to ../output/5746_1.csv\n",
      "output to ../output/5746_0.csv\n",
      "output to ../output/5789_1.csv\n",
      "output to ../output/5789_0.csv\n",
      "output to ../output/5817_1.csv\n",
      "output to ../output/5817_0.csv\n",
      "output to ../output/5859_1.csv\n",
      "output to ../output/5859_0.csv\n",
      "output to ../output/5967_1.csv\n",
      "output to ../output/5967_0.csv\n",
      "output to ../output/6201_0.csv\n",
      "output to ../output/6201_1.csv\n",
      "output to ../output/6203_1.csv\n",
      "output to ../output/6203_0.csv\n",
      "output to ../output/6216_1.csv\n",
      "output to ../output/6216_0.csv\n",
      "output to ../output/6220_1.csv\n",
      "output to ../output/6220_0.csv\n",
      "output to ../output/6292_1.csv\n",
      "output to ../output/6292_0.csv\n",
      "output to ../output/6400_1.csv\n",
      "output to ../output/6400_0.csv\n",
      "output to ../output/4868_1.csv\n",
      "output to ../output/4868_0.csv\n",
      "S value before a P value at 5021\n",
      "S value before a P value at 5021\n",
      "output to ../output/5021_0.csv\n",
      "S value before a P value at 5021\n",
      "S value before a P value at 5021\n",
      "output to ../output/5021_1.csv\n",
      "output to ../output/5022_1.csv\n",
      "output to ../output/5022_0.csv\n",
      "output to ../output/5027_1.csv\n",
      "output to ../output/5027_0.csv\n",
      "output to ../output/5039_1.csv\n",
      "output to ../output/5039_0.csv\n",
      "output to ../output/5040_1.csv\n",
      "output to ../output/5040_0.csv\n",
      "output to ../output/5047_0.csv\n",
      "output to ../output/5047_1.csv\n",
      "output to ../output/5090_0.csv\n",
      "output to ../output/5090_1.csv\n",
      "output to ../output/5143_0.csv\n",
      "output to ../output/5143_1.csv\n",
      "output to ../output/5155_1.csv\n",
      "output to ../output/5155_0.csv\n",
      "output to ../output/5232_1.csv\n",
      "output to ../output/5232_0.csv\n",
      "output to ../output/5273_1.csv\n",
      "output to ../output/5273_0.csv\n",
      "output to ../output/5301_1.csv\n",
      "output to ../output/5301_0.csv\n",
      "output to ../output/5394_0.csv\n",
      "output to ../output/5394_1.csv\n",
      "output to ../output/5398_1.csv\n",
      "output to ../output/5398_0.csv\n",
      "output to ../output/5404_1.csv\n",
      "output to ../output/5404_0.csv\n",
      "output to ../output/5832_1.csv\n",
      "output to ../output/5832_0.csv\n",
      "output to ../output/6060_1.csv\n",
      "output to ../output/6060_0.csv\n",
      "output to ../output/6245_1.csv\n",
      "output to ../output/6245_0.csv\n",
      "output to ../output/6299_1.csv\n",
      "output to ../output/6299_0.csv\n",
      "output to ../output/6319_1.csv\n",
      "output to ../output/6319_0.csv\n",
      "output to ../output/6522_0.csv\n"
     ]
    }
   ],
   "source": [
    "initialize()\n",
    "compare_files('../output')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
